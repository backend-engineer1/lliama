{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
   "metadata": {},
   "source": [
    "# Weaviate Vector Store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "#### Creating a Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ad68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccceb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a4b618-668d-4713-84c5-6362030e9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8b27e5-5ad5-4dfe-90c7-0cf1f1d1b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud\n",
    "resource_owner_config = weaviate.AuthClientPassword(\n",
    "    username=\"<username>\",\n",
    "    password=\"<password>\",\n",
    ")\n",
    "client = weaviate.Client(\n",
    "    \"https://llama-test-ezjahb4m.weaviate.network\",\n",
    "    auth_client_secret=resource_owner_config,\n",
    ")\n",
    "\n",
    "# local\n",
    "# client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "#### Load documents, build the VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2bcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "# If you want to load the index later, be sure to give it a name!\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"LlamaIndex\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "\n",
    "# NOTE: you may also choose to define a index_name manually.\n",
    "# index_name = \"test_prefix\"\n",
    "# vector_store = WeaviateVectorStore(weaviate_client=client, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "#### Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35369eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Growing up, the author wrote short stories, experimented with programming on an IBM 1401, nagged his father to buy a TRS-80 computer, wrote simple games, a program to predict how high his model rockets would fly, and a word processor. He also studied philosophy in college, switched to AI, and worked on building the infrastructure of the web. He wrote essays and published them online, had dinners for a group of friends every Thursday night, painted, and bought a building in Cambridge.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0123088b",
   "metadata": {},
   "source": [
    "## Loading the index\n",
    "\n",
    "Here, we use the same index name as when we created the initial index. This stops it from being auto-generated and allows us to easily connect back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e2c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_owner_config = weaviate.AuthClientPassword(\n",
    "    username=\"<username>\",\n",
    "    password=\"<password>\",\n",
    ")\n",
    "client = weaviate.Client(\n",
    "    \"https://llama-test-ezjahb4m.weaviate.network\",\n",
    "    auth_client_secret=resource_owner_config,\n",
    ")\n",
    "\n",
    "# local\n",
    "# client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e0997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=\"LlamaIndex\")\n",
    "\n",
    "loaded_index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc9a2ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "At Interleaf, a group of people worked on projects for customers. One of the employees told the narrator about a new thing called HTML, which was a derivative of SGML. The narrator left Interleaf to go back to RISD and did freelance work for the group that did projects for customers. Later, the narrator and a college friend started a new company called Viaweb, which was a web app that allowed users to build stores through the browser. They got seed funding and recruited two programmers to help them build the software. They opened for business in January 1996 with 6 stores.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = loaded_index.as_query_engine()\n",
    "response = query_engine.query(\"What happened at interleaf?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a41a70d",
   "metadata": {},
   "source": [
    "## Metadata Filtering\n",
    "\n",
    "Let's insert a dummy document, and try to filter so that only that document is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df6b6d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'README.md', 'category': 'codebase'}\n",
      "-----\n",
      "\n",
      "Context\n",
      "LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. \n",
      "They a\n"
     ]
    }
   ],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "doc = Document.example()\n",
    "print(doc.metadata)\n",
    "print(\"-----\")\n",
    "print(doc.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b0b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_index.insert(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bd18f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "The name of the file is README.md.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters\n",
    "\n",
    "filters = MetadataFilters(filters=[ExactMatchFilter(key=\"filename\", value=\"README.md\")])\n",
    "query_engine = loaded_index.as_query_engine(filters=filters)\n",
    "response = query_engine.query(\"What is the name of the file?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d66cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
