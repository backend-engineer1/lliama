{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Vector Stores - Maximum Marginal Relevance Retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of MMR retrieval [<a href=\"https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf\">1</a>]. By using maximum marginal relevance, one can iteratively find documents that are dissimilar to previous results. It has been shown to improve performance for LLM retrievals [<a href=\"https://arxiv.org/pdf/2211.13892.pdf\">2</a>]. \n",
    "\n",
    "The maximum marginal relevance algorithm is as follows:\n",
    "$$\n",
    "\\text{{MMR}} = \\arg\\max_{d_i \\in D \\setminus R} [ \\lambda \\cdot Sim_1(d_i, q) - (1 - \\lambda) \\cdot \\max_{d_j \\in R} Sim_2(d_i, d_j) ]\n",
    "$$\n",
    "\n",
    "Here, D is the set of all candidate documents, R is the set of already selected documents, q is the query, $Sim_1$ is the similarity function between a document and the query, and $Sim_2$ is the similarity function between two documents. $d_i$ and $d_j$ are documents in D and R respectively.\n",
    "\n",
    "The parameter Î» (mmr_threshold) controls the trade-off between relevance (the first term) and diversity (the second term). If mmr_threshold is close to 1, more emphasis is put on relevance, while a mmr_threshold close to 0 puts more emphasis on diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The author grew up writing essays on topics they had stacked up, exploring other things they could work on, and learning Italian. They lived in Florence, Italy and experienced the city at street level in all conditions. They also studied art and painting, and became familiar with the signature style seekers at RISD. They later moved to Cambridge, Massachusetts and got an apartment that was rent-stabilized. They worked on software, including a code editor and an online store builder, and wrote essays about their experiences. They also founded Y Combinator, a startup accelerator, and created the Summer Founders Program to give undergrads an alternative to working at tech companies.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "# llama_index/docs/examples/data/paul_graham\n",
    "documents = SimpleDirectoryReader('../data/paul_graham/').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# To use mmr, set it as a vector_store_query_mode\n",
    "query_engine = index.as_query_engine(vector_store_query_mode=\"mmr\")\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The author grew up writing essays on topics they had stacked up, exploring other things they could work on, and learning Italian. They lived in Florence, Italy and experienced the city at street level in all conditions. They also studied art and painting, and became familiar with the signature style seekers at RISD. They later moved to Cambridge, Massachusetts and got an apartment that was rent-stabilized. They worked on software, including a code editor and an online store builder, and wrote essays about their experiences. They also founded Y Combinator, a startup accelerator, and developed the batch model of funding startups.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('../data/paul_graham/').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# To set the threshold, set it in vector_store_kwargs\n",
    "query_engine_with_threshold = index.as_query_engine(\n",
    "    vector_store_query_mode=\"mmr\", vector_store_kwargs={\"mmr_threshold\":0.2}\n",
    ")\n",
    "\n",
    "response = query_engine_with_threshold.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the node score will be scaled with the threshold and will additionally be penalized for the similarity to previous nodes. As the threshold goes to 1, the scores will become equal and similarity to previous nodes will be ignored, turning off the impact of MMR. By lowering the threshold, the algorithm will prefer more diverse documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores without MMR  [0.8139363671956625, 0.8110763805571549]\n",
      "Scores with MMR and a threshold of 0.8  [0.6511610127407832, 0.4716293734403398]\n",
      "Scores with MMR and a threshold of 0.2  [0.16278861260228436, -0.4745776806511904]\n"
     ]
    }
   ],
   "source": [
    "index1 = VectorStoreIndex.from_documents(documents)\n",
    "query_engine_no_mrr = index1.as_query_engine()\n",
    "response_no_mmr = query_engine_no_mrr.query(\"What did the author do growing up?\")\n",
    "\n",
    "index2 = VectorStoreIndex.from_documents(documents)\n",
    "query_engine_with_high_threshold = index2.as_query_engine(\n",
    "    vector_store_query_mode=\"mmr\", vector_store_kwargs={\"mmr_threshold\":0.8}\n",
    ")\n",
    "response_low_threshold = query_engine_with_low_threshold.query(\"What did the author do growing up?\")\n",
    "\n",
    "index3 = VectorStoreIndex.from_documents(documents)\n",
    "query_engine_with_low_threshold = index3.as_query_engine(\n",
    "    vector_store_query_mode=\"mmr\", vector_store_kwargs={\"mmr_threshold\":0.2}\n",
    ")\n",
    "response_high_threshold = query_engine_with_high_threshold.query(\"What did the author do growing up?\")\n",
    "\n",
    "print(\"Scores without MMR \", [node.score for node in response_no_mmr.source_nodes])\n",
    "print(\"Scores with MMR and a threshold of 0.8 \", [node.score for node in response_high_threshold.source_nodes])\n",
    "print(\"Scores with MMR and a threshold of 0.2 \", [node.score for node in response_low_threshold.source_nodes])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_llama_index",
   "language": "python",
   "name": "venv_llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
