{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05",
         "metadata": {},
         "source": [
            "# ChatGPT"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119",
         "metadata": {},
         "source": [
            "#### Load documents, build the GPTVectorStoreIndex"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                  "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
                  "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
                  "NumExpr defaulting to 8 threads.\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            }
         ],
         "source": [
            "import logging\n",
            "import sys\n",
            "\n",
            "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
            "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
            "\n",
            "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from IPython.display import Markdown, display"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "03d1691e-544b-454f-825b-5ee12f7faa8a",
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# load documents\n",
            "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "6cc980e8-f4e1-4fad-93f8-ab1bbaa874f3",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
                  "Unknown max input size for gpt-3.5-turbo, using defaults.\n"
               ]
            }
         ],
         "source": [
            "# LLM Predictor (gpt-3.5-turbo) + service context\n",
            "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", streaming=True))\n",
            "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58",
         "metadata": {
            "scrolled": true,
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                  "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 27212 tokens\n",
                  "> [build_index_from_nodes] Total embedding token usage: 27212 tokens\n"
               ]
            }
         ],
         "source": [
            "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4",
         "metadata": {},
         "source": [
            "#### Query Index"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "83e2905e-3789-4793-82b9-0ac488246824",
         "metadata": {},
         "source": [
            "By default, with the help of langchain's PromptSelector abstraction, we use \n",
            "a modified refine prompt tailored for ChatGPT-use if the ChatGPT model is used."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
         "metadata": {
            "scrolled": true,
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                  "> [retrieve] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n",
                  "> [retrieve] Total embedding token usage: 8 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
                  "> [get_response] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                  "> [get_response] Total embedding token usage: 0 tokens\n"
               ]
            }
         ],
         "source": [
            "query_engine = index.as_query_engine(\n",
            "    service_context=service_context,\n",
            "    similarity_top_k=3,\n",
            "    streaming=True,\n",
            ")\n",
            "response = query_engine.query(\n",
            "    \"What did the author do growing up?\", \n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "b0262da8",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Before college, the author worked on writing and programming. They wrote short stories and tried writing programs on an IBM 1401 using an early version of Fortran. With microcomputers, the author's interest in programming grew, and they eventually created a new dialect of Lisp called Arc. They also started publishing essays online and realized the potential of the internet for publishing written work."
               ]
            }
         ],
         "source": [
            "response.print_response_stream()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "ec88df57",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                  "> [retrieve] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
                  "> [retrieve] Total embedding token usage: 12 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
                  "> [get_response] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                  "> [get_response] Total embedding token usage: 0 tokens\n"
               ]
            }
         ],
         "source": [
            "query_engine = index.as_query_engine(\n",
            "    service_context=service_context,\n",
            "    similarity_top_k=5,\n",
            "    streaming=True,\n",
            ")\n",
            "response = query_engine.query(\n",
            "    \"What did the author do during his time at RISD?\", \n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "67e8e675-1b03-423a-b53e-23ab278ba03b",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "The author attended RISD and took classes in fundamental subjects like drawing, color, and design. They also learned a lot in the color class they took, but otherwise, they were basically teaching themselves to paint. The author dropped out of RISD in 1993."
               ]
            }
         ],
         "source": [
            "response.print_response_stream()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "88ca1808-d112-4c28-b110-b65dcc9b7207",
         "metadata": {},
         "source": [
            "**Refine Prompt**: Here is the chat refine prompt "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "id": "2f0c270d-9de5-40bf-88fc-83a360523db0",
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from llama_index.prompts.chat_prompts import CHAT_REFINE_PROMPT"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "id": "4db38651-9790-4a61-ac3d-689ce6dfa369",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'input_variables': ['existing_answer', 'context_msg', 'query_str'],\n",
                     " 'output_parser': None,\n",
                     " 'partial_variables': {},\n",
                     " 'messages': [HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query_str'], output_parser=None, partial_variables={}, template='{query_str}', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
                     "  AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['existing_answer'], output_parser=None, partial_variables={}, template='{existing_answer}', template_format='f-string', validate_template=True), additional_kwargs={}),\n",
                     "  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_msg'], output_parser=None, partial_variables={}, template=\"We have the opportunity to refine the above answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.\", template_format='f-string', validate_template=True), additional_kwargs={})]}"
                  ]
               },
               "execution_count": 20,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "dict(CHAT_REFINE_PROMPT.prompt)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6cb664e8-f53f-4d6c-a086-1f2784cc1dc8",
         "metadata": {},
         "source": [
            "#### Query Index (Using the standard Refine Prompt)\n",
            "\n",
            "If we use the \"standard\" refine prompt (where the prompt is one text template instead of multiple messages), we find that the results over ChatGPT are worse. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "id": "29c416f8-d5ab-47d6-8b16-f615bfa58219",
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "from llama_index.prompts.default_prompts import DEFAULT_REFINE_PROMPT"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "id": "3df1acc4-735a-48ac-9fb4-73d9d7eabc02",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                  "> [retrieve] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
                  "> [retrieve] Total embedding token usage: 12 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
                  "> [get_response] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                  "> [get_response] Total embedding token usage: 0 tokens\n"
               ]
            }
         ],
         "source": [
            "query_engine = index.as_query_engine(\n",
            "    service_context=service_context,\n",
            "    refine_template=DEFAULT_REFINE_PROMPT,\n",
            "    similarity_top_k=5,\n",
            "    streaming=True,\n",
            ")\n",
            "response = query_engine.query(\n",
            "    \"What did the author do during his time at RISD?\", \n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "id": "b8938077-6527-4008-8d0c-af7a8178ff10",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "The author attended RISD and took classes in fundamental subjects like drawing, color, and design. They also learned a lot in the color class they took, but otherwise, they were basically teaching themselves to paint. The author dropped out of RISD in 1993.\n"
               ]
            }
         ],
         "source": [
            "response.print_response_stream()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2e024521-97b5-417f-8c27-950983f52cda",
         "metadata": {},
         "source": [
            "### [Beta] Use ChatGPTLLMPredictor\n",
            "\n",
            "Very simple GPT-Index-native ChatGPT wrapper. Note: this is a beta feature. If this doesn't work please\n",
            "use the suggested flow above."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "id": "a49d9a1b-21fb-4153-ad24-191a13513d64",
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# use ChatGPT [beta]\n",
            "from llama_index.llm_predictor.chatgpt import ChatGPTLLMPredictor\n",
            "from langchain.prompts.chat import SystemMessagePromptTemplate\n",
            "\n",
            "llm_predictor = ChatGPTLLMPredictor()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "id": "596af2aa-7ddf-41f2-801b-4a24a4980dd8",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
                  "> [retrieve] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
                  "> [retrieve] Total embedding token usage: 12 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n",
                  "> [get_response] Total LLM token usage: 0 tokens\n",
                  "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
                  "> [get_response] Total embedding token usage: 0 tokens\n"
               ]
            }
         ],
         "source": [
            "query_engine = index.as_query_engine(\n",
            "    service_context=service_context,\n",
            "    streaming=True,\n",
            ")\n",
            "response = query_engine.query(\n",
            "    \"What did the author do during his time at RISD?\", \n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "id": "771e20ba-ccba-447e-89d6-8d731accc6f3",
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "The author did the foundation program at RISD, which included classes in fundamental subjects like drawing, color, and design."
               ]
            }
         ],
         "source": [
            "response.print_response_stream()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "829bb58d-910e-4a19-bbea-8a9546d24b92",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "llama",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
