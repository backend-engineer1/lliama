{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cea58c-48bc-4af6-8358-df9695659983",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build your own OpenAI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {
    "tags": []
   },
   "source": [
    "With the [new OpenAI API](https://openai.com/blog/function-calling-and-other-api-updates) that supports function calling, it's never been easier to build your own agent!\n",
    "\n",
    "In this notebook tutorial, we showcase how to write your own OpenAI agent in **under 50 lines of code**! It is minimal, yet feature complete (with ability to carry on a conversation and use tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7bc2e-606f-411a-9490-fcfab9236dfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initial Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80e5b-aaee-4f23-b338-7ae62b08141f",
   "metadata": {},
   "source": [
    "Let's start by importing some simple building blocks.  \n",
    "\n",
    "The main thing we need is:\n",
    "1. the OpenAI API (we will use langchain's ChatOpenAI wrapper for convienience here.)\n",
    "2. a place to keep conversation history \n",
    "3. a definition for tools that our agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema import FunctionMessage\n",
    "from llama_index.tools import BaseTool, FunctionTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe08eb1-e638-4c00-9103-5c305bfacccf",
   "metadata": {},
   "source": [
    "Let's define some very simple calculator tools for our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd3c4a6-f3e0-46f9-ad3b-7ba57d1bc992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfcfb78b-7d4f-48d9-8d4c-ffcded23e7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbd5ea-f377-44a0-a492-4568daa8b0b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b737e6c-64eb-4ae6-a8f7-350b1953e612",
   "metadata": {},
   "source": [
    "Now, we define our agent that's capable of holding a conversation and calling tools in **under 50 lines of code**.\n",
    "\n",
    "The meat of the agent logic is in the `chat` method. At a high-level, there are 3 steps:\n",
    "1. Call OpenAI to decide which tool (if any) to call and with what arguments.\n",
    "2. Call the tool with the arguments to obtain an output\n",
    "3. Call OpenAI to synthesize a response from the conversation context and the tool output.\n",
    "\n",
    "The `reset` method simply resets the conversation context, so we can start another conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e068f7-fd24-4f74-8243-5e6e4840f7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YourOpenAIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: Sequence[BaseTool] = [],\n",
    "        llm: ChatOpenAI = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0613\"),\n",
    "        chat_history: ChatMessageHistory = ChatMessageHistory(),\n",
    "    ) -> None:\n",
    "        self._llm = llm\n",
    "        self._tools = {tool.metadata.name: tool for tool in tools}\n",
    "        self._chat_history = chat_history\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._chat_history.clear()\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        chat_history = self._chat_history\n",
    "        chat_history.add_user_message(message)\n",
    "        functions = [tool.metadata.to_openai_function() for _, tool in self._tools.items()]\n",
    "\n",
    "        ai_message = self._llm.predict_messages(chat_history.messages, functions=functions)\n",
    "        chat_history.add_message(ai_message)\n",
    "\n",
    "        function_call = ai_message.additional_kwargs.get(\"function_call\", None)\n",
    "        if function_call is not None:\n",
    "            function_message = self._call_function(function_call)\n",
    "            chat_history.add_message(function_message)\n",
    "            ai_message = self._llm.predict_messages(chat_history.messages)\n",
    "            chat_history.add_message(ai_message)\n",
    "\n",
    "        return ai_message.content\n",
    "\n",
    "    def _call_function(self, function_call: dict) -> FunctionMessage:\n",
    "        tool = self._tools[function_call[\"name\"]]\n",
    "        output = tool(**json.loads(function_call[\"arguments\"]))\n",
    "        return FunctionMessage(\n",
    "            name=function_call[\"name\"],\n",
    "            content=str(output), \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2cec5-6cc0-4814-92a1-ca0bd237528f",
   "metadata": {},
   "source": [
    "## Let's Try It Out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08928f6e-610c-420b-8a7b-7a7042bbd6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = YourOpenAIAgent(tools=[multiply_tool, add_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5cefbad-32c4-4273-807a-cc179bae4473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f7650d-57b8-4ef4-b19d-651281ddb1be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product of 2123 multiplied by 215123 is 456,706,129.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat('What is 2123 * 215123')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d30b8-6405-4187-a9ed-6146dcc42167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Our (Slightly Better) `OpenAIAgent` Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ca3fd-6711-4c0c-a853-d868dd14b484",
   "metadata": {},
   "source": [
    "We provide a (slightly better) `OpenAIAgent` implementation in LlamaIndex, which you can directly use as follows.  \n",
    "\n",
    "In comparison to the simplified version above:\n",
    "* it implements the `BaseChatEngine` and `BaseQueryEngine` interface, so you can more seamlessly use it in the LlamaIndex framework. \n",
    "* it supports multiple function calls per conversation turn\n",
    "* it supports async endpoints\n",
    "* it supports callback and tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ab3938-1138-43ea-b085-f430b42f5377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d852ece7-e5a1-4368-9d59-c7014e0b5b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools([multiply_tool, add_tool], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33ea069f-819b-4ec1-a93c-fcbaacb362a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Human: What's 212 * 122 + 213. Make sure to use tools for any calculation\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 212,\n",
      "  \"b\": 122\n",
      "}\n",
      "Got output: 25864\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\n",
      "  \"a\": 25864,\n",
      "  \"b\": 213\n",
      "}\n",
      "Got output: 26077\n",
      "========================\n",
      "Assistant: The result of 212 * 122 + 213 is 26077.\n",
      "\n",
      "Human: exit\n"
     ]
    }
   ],
   "source": [
    "agent.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4199b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index",
   "language": "python",
   "name": "llama-index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
