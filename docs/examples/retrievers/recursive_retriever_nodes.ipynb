{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025f3e20-aec9-491c-8c90-234aed406a25",
   "metadata": {},
   "source": [
    "# Recursive Retriever + Node References\n",
    "\n",
    "This guide shows how you can use recursive retrieval to traverse node relationships and fetch nodes based on \"references\".\n",
    "\n",
    "Node references are a powerful concept. When you first perform retrieval, you may want to retrieve the reference as opposed to the raw text. You can have multiple references point to the same node.\n",
    "\n",
    "In this guide we explore some different usages of node references:\n",
    "- Different chunk sizes referring to a bigger chunk\n",
    "- TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb829ef-b54b-4095-a832-6d1d115aa645",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd97455-5ff3-43ee-8222-f496ec234dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pdf.base import PDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07c0e42-1ae8-4267-9355-6bb75323f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFReader()\n",
    "docs0 = loader.load_data(file=Path(\"./data/llama2.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493e5492-a6ae-4e3e-aa23-274c0605b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "docs = [Document(text=doc_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5431df3-d255-4492-bce4-bbebde6f2306",
   "metadata": {},
   "source": [
    "## Node References: Smaller Child Chunks Referring to Bigger Parent Chunk\n",
    "\n",
    "In this usage example, we show how to build a graph of smaller chunks pointing to bigger parent chunks.\n",
    "\n",
    "During query-time, we retrieve smaller chunks, but we follow references to bigger chunks. This allows us to have more context for synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c6e216-019a-4fe5-aaa5-83ecd109e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import IndexNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90843943-27f6-4168-9fb5-37c9db761cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af8e44a-f449-4f2e-a09a-f0059748635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes = node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3102d7de-d94c-428b-b2df-d37565d4f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom∗\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases.Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
      "source models.We provide a detailed description of our approach to fine-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "†Second author\n",
      "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "\n",
      "Contents\n",
      "1 Introduction 3\n",
      "2 Pretraining 5\n",
      "2.1 Pretraining Data .............................................5\n",
      "2.2 Training Details .............................................5\n",
      "2.3 Llama 2 Pretrained Model Evaluation ................................7\n",
      "3 Fine-tuning 8\n",
      "3.1 Supervised Fine-Tuning (SFT) .....................................9\n",
      "3.2 Reinforcement Learning with Human Feedback (RLHF) .....................9\n",
      "3.3 System Message for Multi-Turn Consistency .............................16\n",
      "3.4 RLHF Results ..............................................17\n",
      "4 Safety 20\n",
      "4.1 Safety in Pretraining ..........................................20\n",
      "4.2 Safety Fine-Tuning ...........................................23\n",
      "4.3 Red Teaming ........\n"
     ]
    }
   ],
   "source": [
    "print(base_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c784d8-71e6-42bc-84d9-a2aea4217b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_chunk_sizes = [128, 256, 512]\n",
    "sub_node_parsers = [\n",
    "    SimpleNodeParser.from_defaults(chunk_size=c) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    base_inode = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(base_inode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d614088-b122-40ad-811a-29cc0c2a295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_dict = {n.node_id: n for n in all_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a44ef2d5-0342-4073-831f-f35dd6f04dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index in a vector db\n",
    "\n",
    "## Load index into vector index\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "vector_index = VectorStoreIndex(all_nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c06af99f-02be-4055-a6ea-3071ffe8fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d56cb2-eb51-46e0-9b3a-9400b48c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_retriever.retrieve(\n",
    "    \"Can you tell me about the key concepts for safety finetuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd92d303-208b-4c0d-897a-66079337db48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=IndexNode(id_='a0782caf-0763-4a73-b8ec-1035d223887d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4c404d8f-95dd-4205-b7c2-83ad151f9ed7', node_type=None, metadata={}, hash='e91a5d68ee74c66ce0e0c49fd32a5f6a17c4fbc26a6305919029a754f32c17d0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='598481b3-fb73-4afc-bcc9-af8312494031', node_type=None, metadata={}, hash='968e147841df2e03a009747584987b0ccf284b1e79f467e5838fbfae38aa9f4a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='51877384-dcca-4b70-9c3e-e9c0f16e0eb1', node_type=None, metadata={}, hash='1891e6c506d2ba476704c866e98906bea6621dd70f80213fa00ac3a8308fc597')}, hash='8adbc43ed86fb0e6e8727d401e4608bd66ef957246e865002bfb8efc1ff8d5b7', text='andthetechniquesweusetomitigatesafetyrisks.Weemployaprocesssimilartothegeneral\\nfine-tuning methods as described in Section 3, with some notable differences related to safety concerns.Specifically, we use the following techniques in safety fine-tuning:\\n1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\\ntions that are then included in the general supervised fine-tuning process (Section 3.1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='4c404d8f-95dd-4205-b7c2-83ad151f9ed7'), score=0.8693572142828401),\n",
       " NodeWithScore(node=IndexNode(id_='3d6df568-6195-4e4b-b476-b1eaec79a8dc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='61c5f66a-77e9-4c28-8310-028cb3461ed1', node_type=None, metadata={}, hash='9ee8e31806fe800ec634337038bd05e9677d9fe229c0019520db088d0d882c05'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a5dd8e2d-8e64-4867-818a-fccc5a38afb8', node_type=None, metadata={}, hash='17f4d945ac1cc1111320117bce7399c45288d1663ee34036e81263afaeab174f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2d29e350-a27f-4218-acc2-af5283380145', node_type=None, metadata={}, hash='8adf91ed863e39fa8ea8038182620175ffd8fb0bce1063688bb8a231149c996a')}, hash='e536b5d462a207cebd3701e33133b1de16437a6df81e7c585327a711fbb4a31e', text='The guidelines are meant to be a general guide for the model and are\\niteratively refined and revised to include newly identified risks.4.2.2 Safety Supervised Fine-Tuning\\nInaccordancewiththeestablishedguidelinesfromSection4.2.1,wegatherpromptsanddemonstrations\\nofsafemodelresponsesfromtrainedannotators,andusethedataforsupervisedfine-tuninginthesame\\nmanner as described in Section 3.1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='61c5f66a-77e9-4c28-8310-028cb3461ed1'), score=0.854811252011206)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fefee876-eba6-4768-b80b-36a23a52253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import RecursiveRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c7c5e43-45b5-42d6-afc5-cb81ed3cb211",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e9f7bcb-5442-4d2d-a7eb-814b68ebb45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mRetrieving with query id None: Can you tell me about the key concepts for safety finetuning\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mRetrieved node with id, entering: 4c404d8f-95dd-4205-b7c2-83ad151f9ed7\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRetrieving with query id 4c404d8f-95dd-4205-b7c2-83ad151f9ed7: Can you tell me about the key concepts for safety finetuning\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mRetrieved node with id, entering: 61c5f66a-77e9-4c28-8310-028cb3461ed1\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mRetrieving with query id 61c5f66a-77e9-4c28-8310-028cb3461ed1: Can you tell me about the key concepts for safety finetuning\n",
      "\u001b[0m22\n",
      "\n",
      "TruthfulQA ↑ToxiGen ↓\n",
      "MPT7B 29.13 22.32\n",
      "30B 35.25 22.61\n",
      "Falcon7B 25.95 14.53\n",
      "40B 40.39 23.44\n",
      "Llama 17B 27.42 23.00\n",
      "13B 41.74 23.08\n",
      "33B 44.19 22.57\n",
      "65B 48.71 21.77\n",
      "Llama 27B 33.29 21.25\n",
      "13B 41.86 26.10\n",
      "34B 43.45 21.19\n",
      "70B 50.18 24.60\n",
      "Table 11: Evaluation of pretrained LLMs on automatic safety benchmarks.For TruthfulQA, we present the\n",
      "percentageofgenerationsthatarebothtruthfulandinformative(thehigherthebetter).ForToxiGen,we\n",
      "present the percentage of toxic generations (the smaller, the better).Benchmarks give a summary view ofmodel capabilities and behaviors that allow us to understand general\n",
      "patternsinthemodel,buttheydonotprovideafullycomprehensiveviewoftheimpactthemodelmayhave\n",
      "onpeopleorreal-worldoutcomes;thatwouldrequirestudyofend-to-endproductdeployments.Further\n",
      "testing and mitigation should be done to understand bias and other social issues for the specific context\n",
      "in which a system may be deployed.For this, it may be necessary to test beyond the groups available in\n",
      "theBOLDdataset(race,religion,andgender).AsLLMsareintegratedanddeployed,welookforwardto\n",
      "continuing research that will amplify their potential for positive impact on these important social issues.4.2 Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines,andthetechniquesweusetomitigatesafetyrisks.Weemployaprocesssimilartothegeneral\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.Specifically, we use the following techniques in safety fine-tuning:\n",
      "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n",
      "tions that are then included in the general supervised fine-tuning process (Section 3.1).This teaches\n",
      "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n",
      "high-quality human preference data annotation.2.Safety RLHF : Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n",
      "tion 3.2.2.This includes training a safety-specific reward model and gathering more challenging\n",
      "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.3.SafetyContextDistillation : Finally,werefineourRLHFpipelinewithcontextdistillation(Askell\n",
      "etal.,2021b).Thisinvolvesgeneratingsafermodelresponsesbyprefixingapromptwithasafety\n",
      "preprompt, e.g., “You are a safe and responsible assistant,” and then fine-tuning the model on the safer\n",
      "responses without the preprompt, which essentially distillsthe safety preprompt (context) into the\n",
      "model.Weuseatargetedapproachthatallowsoursafetyrewardmodeltochoosewhethertouse\n",
      "context distillation for each sample.4.2.1 Safety Categories and Annotation Guidelines\n",
      "Based on limitations of LLMs known from prior work, we design instructions for our annotation team to\n",
      "createadversarialpromptsalongtwodimensions: a riskcategory ,orpotentialtopicaboutwhichtheLLM\n",
      "couldproduceunsafecontent;andan attackvector ,orquestionstyletocoverdifferentvarietiesofprompts\n",
      "that could elicit bad model behaviors.Theriskcategoriesconsideredcanbebroadlydividedintothefollowingthreecategories: illicitandcriminal\n",
      "activities (e.g.,terrorism,theft,humantrafficking); hatefulandharmfulactivities (e.g.,defamation,self-\n",
      "harm, eating disorders, discrimination); and unqualified advice (e.g., medical advice, financial advice, legal\n",
      "23\n",
      "\n",
      "advice).The attackvectors exploredconsist ofpsychological manipulation(e.g., authoritymanipulation),\n",
      "logic manipulation (e.g., false premises), syntactic manipulation (e.g., misspelling), semantic manipulation\n",
      "(e.g., metaphor), perspective manipulation (e.g., role playing), non-English languages, and others.\n",
      "Wethendefinebestpracticesforsafeandhelpfulmodelresponses: themodelshouldfirstaddressimmediate\n",
      "safetyconcernsifapplicable,thenaddressthepromptbyexplainingthepotentialriskstotheuser,andfinally\n",
      "provide additional information if possible.We also ask the annotators to avoid negative user experience\n",
      "categories (see Appendix A.5.2).The guidelines are meant to be a general guide for the model and are\n",
      "iteratively refined and revised to include newly identified risks.4.2.2 Safety Supervised Fine-Tuning\n",
      "InaccordancewiththeestablishedguidelinesfromSection4.2.1,wegatherpromptsanddemonstrations\n",
      "ofsafemodelresponsesfromtrainedannotators,andusethedataforsupervisedfine-tuninginthesame\n",
      "manner as described in Section 3.1.An example can be found in Table 5.The annotators are instructed to initially come up with prompts that they think could potentially induce\n",
      "themodel toexhibit unsafebehavior, i.e.,perform redteaming, asdefined bythe guidelines.Subsequently,\n",
      "annotators are tasked with crafting a safe and helpful response that the model should produce.4.2.3 Safety RLHF\n",
      "Weobserveearlyinthedevelopmentof Llama 2-Chat thatitisabletogeneralizefromthesafedemonstrations\n",
      "insupervisedfine-tuning.Themodelquicklylearnstowritedetailedsaferesponses,addresssafetyconcerns,\n",
      "explainwhythetopicmightbesensitive,andprovideadditionalhelpfulinformation.Inparticular,when\n",
      "the model outputs safe responses, they are often more detailed than what the average annotator writes.Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to\n",
      "teachthemodelhowtowritemorenuancedresponses.ComprehensivetuningwithRLHFhastheadded\n",
      "benefit that it may make the model more robust to jailbreak attempts (Bai et al., 2022a).WeconductRLHFbyfirstcollectinghumanpreferencedataforsafetysimilartoSection3.2.2: annotators\n",
      "writeapromptthattheybelievecanelicitunsafebehavior,andthencomparemultiplemodelresponsesto\n",
      "theprompts,selectingtheresponsethatissafestaccordingtoasetofguidelines.Wethenusethehuman\n",
      "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
      "sample from the model during the RLHF stage.BetterLong-TailSafetyRobustnesswithoutHurtingHelpfulness Safetyisinherentlyalong-tailproblem,\n",
      "wherethe challengecomesfrom asmallnumber ofveryspecific cases.Weinvestigatetheimpact ofSafety\n",
      "RLHFbytakingtwointermediate Llama 2-Chat checkpoints—onewithoutadversarialpromptsintheRLHF\n",
      "stageandonewiththem—andscoretheirresponsesonourtestsetsusingoursafetyandhelpfulnessreward\n",
      "models.In Figure 14, we plot the score distribution shift of the safety RM on the safety test set (left) and that\n",
      "of the helpfulness RM on the helpfulness test set (right).In the left hand side of the figure, we observe that\n",
      "thedistributionofsafetyRMscoresonthesafetysetshiftstohigherrewardscoresaftersafetytuningwith\n",
      "RLHF,andthatthelongtailofthedistributionnearzerothinsout.Aclearclusterappearsonthetop-left\n",
      "corner suggesting the improvements of model safety.On the right side, we do not observe any gathering\n",
      "patternbelowthe y=xlineontherighthandsideofFigure14,whichindicatesthatthehelpfulnessscore\n",
      "distributionispreservedaftersafetytuningwithRLHF.Putanotherway,givensufficienthelpfulnesstraining\n",
      "data, the addition of an additional stage of safety mitigation does not negatively impact model performance\n",
      "on helpfulness to any notable degradation.A qualitative example is shown in Table 12.ImpactofSafetyDataScaling.AtensionbetweenhelpfulnessandsafetyofLLMshasbeenobservedin\n",
      "previous studies (Bai et al., 2022a).To better understand how the addition of safety training data affects\n",
      "general model performance, especially helpfulness, we investigate the trends in safety data scaling by\n",
      "adjustingtheamountofsafetydatausedintheRLHFstage.\n"
     ]
    }
   ],
   "source": [
    "nodes = recursive_retriever.retrieve(\n",
    "    \"Can you tell me about the key concepts for safety finetuning\"\n",
    ")\n",
    "for node in nodes:\n",
    "    print(node.node.get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
