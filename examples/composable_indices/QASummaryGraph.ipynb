{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54d1c43-4b7f-4917-939f-a964f6f3dafc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa67fa07-1395-4aab-a356-72bdb302f6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d12d766-3ca8-4012-9da2-248be80bb6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gpt_index.composability.joint_qa_summary import QASummaryGraphBuilder\n",
    "from gpt_index import SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
    "from gpt_index.composability import ComposableGraph\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cdaf9d-cfbd-4ced-8d4e-6eef8508224d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader('../paul_graham_essay/data')\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bba68f3-2743-437e-93b6-ce9ba92e40c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:gpt_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "Unknown max input size for gpt-3.5-turbo, using defaults.\n"
     ]
    }
   ],
   "source": [
    "llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4, chunk_size_limit=1024)\n",
    "\n",
    "llm_predictor_chatgpt = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context_chatgpt = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt, chunk_size_limit=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16216dfb-35ea-49ac-b651-2e8a9e423512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "# NOTE: can also specify an existing docstore, service context, summary text, qa_text, etc.\n",
    "graph_builder = QASummaryGraphBuilder(service_context=service_context_gpt4)\n",
    "graph = graph_builder.build_graph_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19834e20-5b11-4b08-920f-d05107359980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = graph.as_query_engine(\n",
    "    response_mode='tree_summarize',\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    service_context=service_context_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae60000b-403c-4350-af32-71e26cc68a75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [2]/[2]\n",
      ">[Level 0] Selected node: [2]/[2]\n",
      "INFO:gpt_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 172 tokens\n",
      "> [retrieve] Total LLM token usage: 172 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.indices.common_tree.base:> Building index from nodes: 6 chunks\n",
      "> Building index from nodes: 6 chunks\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4254 request_id=99860fad320ba2443dcbed71341019cb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4254 request_id=99860fad320ba2443dcbed71341019cb response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=15317 request_id=2d14e999f947131263d969d2c5fa63c2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=15317 request_id=2d14e999f947131263d969d2c5fa63c2 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=20475 request_id=22ce588ea84128f97dc47e7da7ab5748 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=20475 request_id=22ce588ea84128f97dc47e7da7ab5748 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=21386 request_id=57a55835cc7359381c280b0bb6b43858 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=21386 request_id=57a55835cc7359381c280b0bb6b43858 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=22007 request_id=470af9eaba5f082822a76983c6e6a62d response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=22007 request_id=470af9eaba5f082822a76983c6e6a62d response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=23658 request_id=d4e25d9225afe2d5d05bcb1da3aac972 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=23658 request_id=d4e25d9225afe2d5d05bcb1da3aac972 response_code=200\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=36039 request_id=e90414b1fb5f4d21c8399c9f09391f88 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=36039 request_id=e90414b1fb5f4d21c8399c9f09391f88 response_code=200\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1020 tokens\n",
      "> [get_response] Total LLM token usage: 1020 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: 23495 tokens\n",
      "> [get_response] Total LLM token usage: 23495 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: 299 tokens\n",
      "> [get_response] Total LLM token usage: 299 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total LLM token usage: 299 tokens\n",
      "> [get_response] Total LLM token usage: 299 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = query_engine.query(\n",
    "    \"Can you give me a summary of the author's life?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca12b3ac-fcc2-4998-917a-16f568b59623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author's life has been a diverse journey through various interests and pursuits. They began with a passion for programming and writing short stories, studied artificial intelligence in college, and later explored art by attending art schools in Italy and the US. They worked as a freelance Lisp programmer and studio assistant before co-founding Viaweb, an online store builder that was sold to Yahoo. The author then started writing popular essays online and co-founded Y Combinator, a startup accelerator. Their life has spanned fields such as art, software, venture capital, and essay writing, driven by curiosity and independent-mindedness.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4488669d-0f67-48c9-994c-bd7a42498ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 0] Current response: ANSWER: 1\n",
      "\n",
      "The question, \"What did the author do growing up?\" asks for specific context from documents about the author's experiences or activities during their childhood. Choice 1 mentions retrieval of specific context from documents, which is more in line with answering this type of question than summarization queries mentioned in Choice 2.\n",
      "INFO:llama_index.indices.tree.leaf_query:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Node [1] Summary text: Use this index for queries that require retrieval of specific context from documents.\n",
      "\u001b[36;1m\u001b[1;3m> Got node text: Growing up, the author worked on writing short stories and programming on the IBM 1401, a computer used in their school district. They also experimented with programming on a TRS-80 microcomputer, ...\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What did the author do growing up?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1004bedc-f3c9-4643-9e65-f869a08dec32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, the author worked on writing short stories and programming on the IBM 1401 computer. They also experimented with programming on a TRS-80 microcomputer, creating simple games, a model rocket prediction program, and a word processor for their father.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff95db5f-7cbe-4ed7-83ff-27e00b94e7da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">[Level 0] Current response: ANSWER: 1\n",
      "\n",
      "This summary was selected in relation to the question because it involves retrieval of specific context from documents, which is necessary to know what the author did during his time in art school. Choice 2 is focused on summarization queries, which isn't related to the specific detail needed to answer the question.\n",
      "INFO:llama_index.indices.tree.leaf_query:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Node [1] Summary text: Use this index for queries that require retrieval of specific context from documents.\n",
      "\u001b[36;1m\u001b[1;3m> Got node text: The author took classes in fundamental subjects like drawing, color, and design during the foundation program at RISD. He also prepared for the entrance exam at the Accademia di Belli Arti in Flore...\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What did the author do during his time in art school?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d4f291-88fe-4a51-8650-af77ed35766e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During his time in art school, the author took classes in fundamental subjects, such as drawing, color, and design, as part of the foundation program at RISD. He also prepared for the entrance exam at the Accademia di Belli Arti in Florence, which involved learning Italian.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9bf34-d242-4fbd-b67a-1dc99b387a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
