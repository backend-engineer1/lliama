{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2d202140",
            "metadata": {},
            "source": [
                "# Example of using sentence splitter chunking\n",
                "Compare the diff of splitting_1.txt and splitting_2.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "0a23c1a8-71ea-4b6d-ae42-5c1cf4014dff",
            "metadata": {},
            "outputs": [],
            "source": [
                "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
                "from llama_index import SimpleDirectoryReader, Document\n",
                "from gpt_index.utils import globals_helper\n",
                "from langchain.text_splitter import NLTKTextSplitter, SpacyTextSplitter, RecursiveCharacterTextSplitter\n",
                "\n",
                "document = SimpleDirectoryReader('data').load_data()[0]\n",
                "text_splitter_default = TokenTextSplitter() # use default settings\n",
                "text_chunks = text_splitter_default.split_text(document.text)\n",
                "doc_chunks = [Document(t) for t in text_chunks]\n",
                "tokenizer = globals_helper.tokenizer\n",
                "with open('splitting_1.txt', 'w') as f:\n",
                "    for idx, doc in enumerate(doc_chunks):\n",
                "        f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n",
                "\n",
                "from gpt_index.langchain_helpers.text_splitter import SentenceSplitter\n",
                "\n",
                "sentence_splitter = SentenceSplitter()\n",
                "text_chunks = sentence_splitter.split_text(document.text)\n",
                "doc_chunks = [Document(t) for t in text_chunks]\n",
                "with open('splitting_2.txt', 'w') as f:\n",
                "    for idx, doc in enumerate(doc_chunks):\n",
                "        f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n",
                "\n",
                "nltk_splitter = NLTKTextSplitter()\n",
                "text_chunks = nltk_splitter.split_text(document.text)\n",
                "doc_chunks = [Document(t) for t in text_chunks]\n",
                "tokenizer = globals_helper.tokenizer\n",
                "with open('splitting_3.txt', 'w') as f:\n",
                "    for idx, doc in enumerate(doc_chunks):\n",
                "        f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n",
                "\n",
                "# spacy_splitter = SpacyTextSplitter()\n",
                "# text_chunks = spacy_splitter.split_text(document.text)\n",
                "# tokenizer = globals_helper.tokenizer\n",
                "# with open('splitting_4.txt', 'w') as f:\n",
                "#     for idx, doc in enumerate(doc_chunks):\n",
                "#         f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n",
                "\n",
                "# from langchain.text_splitter import TokenTextSplitter\n",
                "# token_text_splitter = TokenTextSplitter()\n",
                "# text_chunks = token_text_splitter.split_text(document.text)\n",
                "# doc_chunks = [Document(t) for t in text_chunks]\n",
                "# tokenizer = globals_helper.tokenizer\n",
                "# with open('splitting_5.txt', 'w') as f:\n",
                "#     for idx, doc in enumerate(doc_chunks):\n",
                "#         f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n",
                "\n",
                "# recursive_splitter = RecursiveCharacterTextSplitter()\n",
                "# text_chunks = recursive_splitter.split_text(document.text)\n",
                "# doc_chunks = [Document(t) for t in text_chunks]\n",
                "# tokenizer = globals_helper.tokenizer\n",
                "# with open('splitting_6.txt', 'w') as f:\n",
                "#     for idx, doc in enumerate(doc_chunks):\n",
                "#         f.write(\"\\n-------\\n\\n{}. Size: {} tokens\\n\".format(idx, len(tokenizer(doc.text))) + doc.text)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
