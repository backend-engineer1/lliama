{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df19606e-d67e-44d2-bed0-4b804e6fc6c3",
   "metadata": {},
   "source": [
    "# Using Token Predictor\n",
    "\n",
    "Using the token predictor, we can predict the token usage of an operation before actually performing it.\n",
    "We do this with the MockLLMPredictor class, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9eb90-335c-4214-8bb6-fd1edbe3ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My OpenAI Key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"INSERT OPENAI KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f7baa-1c0a-430b-981b-83ddca9e71f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicting Usage of GPT Tree Index\n",
    "\n",
    "Here we predict usage of GPTTreeIndex during index construction and querying, without making any LLM calls.\n",
    "\n",
    "NOTE: Predicting query usage before tree is built is only possible with GPTTreeIndex due to the nature of tree traversal. Results will be more accurate if GPTTreeIndex is actually built beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ef16d1-45ef-43ec-9aad-4e44e9bb8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTTreeIndex, MockLLMPredictor, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ecdadc-1403-4bd4-a876-f80e4da911ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11056808-fd7f-4bc6-9348-0605fb4ee668",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = MockLLMPredictor(max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4ba66-9a09-4478-b0a8-dee8645fa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTTreeIndex(documents, llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345433c2-5553-4645-a513-0186b771a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19495\n"
     ]
    }
   ],
   "source": [
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43733ae-af35-46e6-99d9-8ba507acbb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default query\n",
    "response = index.query(\"What did the author do growing up?\", llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba19751-da2d-46af-9f8f-4f42871e65a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5493\n"
     ]
    }
   ],
   "source": [
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324d85b-ae80-48ab-baf0-7dc160dfae46",
   "metadata": {},
   "source": [
    "### Predicting Usage of GPT Keyword Table Index Query\n",
    "\n",
    "Here we build a real keyword table index over the data, but then predict query usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10447805-38db-41b9-a2c6-b0c95437b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTKeywordTableIndex, MockLLMPredictor, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca76e72-5f43-47c1-a9a4-c5c5db4f0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n",
    "index = GPTKeywordTableIndex.load_from_disk('../paul_graham_essay/index_table.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f48870-65d2-4b23-b57e-79082ecb4ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start token ct: 0\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "query keywords: ['author', 'did', 'y', 'combinator', 'after', 'his', 'the', 'what', 'time', 'at', 'do']\n",
      "Extracted keywords: ['combinator']\n",
      "> Querying with idx: 3483810247393006047: of 2016 we moved to England. We wanted our kids...\n",
      "> Querying with idx: 7597483754542696814: people edit code on our server through the brow...\n",
      "> Querying with idx: 7572417251450701751: invited about 20 of the 225 groups to interview...\n",
      "end token ct: 11313\n",
      "> [query] Total token usage: 11313 tokens\n",
      "11313\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = MockLLMPredictor(max_tokens=256)\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", llm_predictor=llm_predictor)\n",
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee4405-05e0-46c2-87bb-64ec63a4c6c1",
   "metadata": {},
   "source": [
    "### Predicting Usage of GPT List Index Query\n",
    "\n",
    "Here we build a real list index over the data, but then predict query usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267f2213-67d1-4241-b73f-f1790661d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTListIndex, MockLLMPredictor, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d553a8b1-7045-4756-9729-df84bd305279",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n",
    "index = GPTListIndex.load_from_disk('../paul_graham_essay/index_list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c99c68-6a23-48ed-aa41-e7af50fef2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start token ct: 0\n",
      "> Starting query: What did the author do after his time at Y Combinator?\n",
      "end token ct: 23941\n",
      "> [query] Total token usage: 23941 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = MockLLMPredictor(max_tokens=256)\n",
    "response = index.query(\"What did the author do after his time at Y Combinator?\", llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8422c5c-af68-4138-a8dd-f6e8d7208c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23941\n"
     ]
    }
   ],
   "source": [
    "print(llm_predictor.last_token_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9e039-8612-4fc9-a631-82d778afdb73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_retrieve_venv",
   "language": "python",
   "name": "gpt_retrieve_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
