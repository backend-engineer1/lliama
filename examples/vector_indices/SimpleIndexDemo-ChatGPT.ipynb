{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05",
   "metadata": {},
   "source": [
    "# Simple Index Demo + ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34da56e-bc3b-433e-b65c-96edea4db5dd",
   "metadata": {},
   "source": [
    "Use a very simple wrapper around the ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119",
   "metadata": {},
   "source": [
    "#### Load documents, build the GPTSimpleVectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from gpt_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor\n",
    "from langchain.llms import OpenAIChat\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d1691e-544b-454f-825b-5ee12f7faa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 18579 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 18579 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 18579 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 18579 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex(documents, chunk_size_limit=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc980e8-f4e1-4fad-93f8-ab1bbaa874f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Predictor (gpt-3.5-turbo)\n",
    "llm_predictor = LLMPredictor(llm=OpenAIChat(temperature=0, model_name=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4",
   "metadata": {},
   "source": [
    "#### Query Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 2044 tokens\n",
      "> [query] Total LLM token usage: 2044 tokens\n",
      "> [query] Total LLM token usage: 2044 tokens\n",
      "> [query] Total LLM token usage: 2044 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n",
      "> [query] Total embedding token usage: 8 tokens\n",
      "> [query] Total embedding token usage: 8 tokens\n",
      "> [query] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "response = index.query(\n",
    "    \"What did the author do growing up?\", \n",
    "    llm_predictor=llm_predictor,\n",
    "    similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdda1b2c-ae46-47cf-91d7-3153e8d0473b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "The author worked on writing and programming before college, including writing short stories and trying programming on an IBM 1401 in 9th grade using an early version of Fortran. They continued to write essays throughout 2020, but in late 2015, they spent three months solely working on programming their interpreter, Bel. They worked on Bel so intensively that they had a decent chunk of the code in their head at any given time and could write more there. They even figured out how to deal with some problem involving continuations while watching their kids play in the tide pools. The author moved to England in the summer of 2016 and continued to work on Bel until it was finally finished in the fall of 2019. After finishing Bel, the author returned to writing essays and reflecting on what to work on next.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88df57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: this may not give you a satisfactory answer! \n",
    "response = index.query(\n",
    "    \"What did the author do during his time at RISD?\", \n",
    "    llm_predictor=llm_predictor,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67e8e675-1b03-423a-b53e-23ab278ba03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The original answer remains relevant and does not need to be refined as the new context does not provide any information about what the author did during his time at RISD.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb664e8-f53f-4d6c-a086-1f2784cc1dc8",
   "metadata": {},
   "source": [
    "#### Query Index (Refine Prompt Tailored to ChatGPT)\n",
    "\n",
    "We use a custom Refine Prompt tailored to the ChatGPT API.\n",
    "It effectively breaks the refine prompt into separate \"conversational\" messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29c416f8-d5ab-47d6-8b16-f615bfa58219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt_index.prompts.chat_prompts import CHAT_REFINE_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1acc4-735a-48ac-9fb4-73d9d7eabc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    \"What did the author do during his time at RISD?\", \n",
    "    llm_predictor=llm_predictor,\n",
    "    refine_template=CHAT_REFINE_PROMPT,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8938077-6527-4008-8d0c-af7a8178ff10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>During his time at RISD, the author took art classes at Harvard while also being in a PhD program in computer science. He also painted and lived in New York, where he became a de facto studio assistant for a painter named Idelle Weber. He wrote a book on Lisp to earn money and imagined himself living frugally off the royalties and spending all his time painting. He also became interested in the World Wide Web and started a company to put art galleries online. However, he dropped out of RISD in 1993 and moved to New York to pursue his career as an artist. He lived in a rent-controlled apartment in Yorkville and taught himself to paint while freelancing as a Lisp hacker to make ends meet.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e024521-97b5-417f-8c27-950983f52cda",
   "metadata": {},
   "source": [
    "### [Beta] Use ChatGPTLLMPredictor\n",
    "\n",
    "Very simple GPT-Index-native ChatGPT wrapper. Note: this is a beta feature. If this doesn't work please\n",
    "use the suggested flow above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49d9a1b-21fb-4153-ad24-191a13513d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ChatGPT [beta]\n",
    "from gpt_index.langchain_helpers.chatgpt import ChatGPTLLMPredictor\n",
    "\n",
    "llm_predictor = ChatGPTLLMPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596af2aa-7ddf-41f2-801b-4a24a4980dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\n",
    "    \"What did the author do during his time at RISD?\", \n",
    "    llm_predictor=llm_predictor\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
